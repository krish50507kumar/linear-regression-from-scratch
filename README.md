# linear-regression-from-scratch
Multivariate linear regression implemented from scratch using vectorized gradient descent in NumPy. Includes manual cost computation, gradient derivation, feature scaling, and convergence analysis.


ğŸ“ˆ Linear Regression from Scratch (NumPy Implementation)
Overview

This repository contains a from-scratch implementation of multivariate linear regression using:

Batch Gradient Descent

Mean Squared Error (MSE) loss

Analytical gradient derivation

Feature standardization

Log-scale convergence visualization

No ML libraries (e.g., scikit-learn) were used.
All computations are fully vectorized using NumPy.

Objective

Minimize the Mean Squared Error:

ğ½
(
ğ‘Š
)
=
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
2
J(W)=
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(y
i
	â€‹

âˆ’
y
^
	â€‹

i
	â€‹

)
2

Where:

ğ‘¦
^
=
ğ‘‹
ğ‘Š
y
^
	â€‹

=XW

The goal is to learn optimal weights 
ğ‘Š
W using gradient descent.

Gradient Derivation

The analytical gradient of MSE with respect to weights:

âˆ‚
ğ½
âˆ‚
ğ‘Š
=
2
ğ‘š
ğ‘‹
ğ‘‡
(
ğ‘‹
ğ‘Š
âˆ’
ğ‘Œ
)
âˆ‚W
âˆ‚J
	â€‹

=
m
2
	â€‹

X
T
(XWâˆ’Y)

Update rule:

ğ‘Š
:
=
ğ‘Š
âˆ’
ğ›¼
âˆ‚
ğ½
âˆ‚
ğ‘Š
W:=Wâˆ’Î±
âˆ‚W
âˆ‚J
	â€‹


Where:

ğ›¼
Î± = learning rate

ğ‘š
m = number of samples

Implementation Highlights

Fully vectorized (no explicit loops over samples)

Bias term handled explicitly

Feature standardization (excluding bias column)

Log-scale cost plot to analyze convergence behavior

Manual weight initialization

Feature Scaling

All features (except bias term) are standardized:

ğ‘‹
ğ‘ 
ğ‘
ğ‘
ğ‘™
ğ‘’
ğ‘‘
=
ğ‘‹
âˆ’
ğœ‡
ğœ
X
scaled
	â€‹

=
Ïƒ
Xâˆ’Î¼
	â€‹


This improves convergence stability and prevents gradient explosion/divergence.

Convergence Behavior

The cost is plotted on a logarithmic scale to visualize exponential decay during early training phases.

Initial rapid decrease occurs due to large gradient magnitude when weights are far from the optimum.

Tech Stack

Python 3

NumPy

Matplotlib

Why This Project?

This implementation was built to:

Understand optimization mechanics at a mathematical level

Develop intuition for gradient-based learning

Avoid black-box ML abstractions

Strengthen linear algebra + numerical computation skills

Possible Extensions

Early stopping criteria

Learning rate scheduling

Mini-batch gradient descent

Normal equation comparison

Regularization (L2 / Ridge)

Author

Krish
Aspiring ML engineer focused on first-principles understanding of machine learning systems.
